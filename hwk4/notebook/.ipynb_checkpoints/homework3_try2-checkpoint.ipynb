{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    DSCI 552 Homework 3<br>\n",
    "    Name: Yuhui Zou<br>\n",
    "    USC ID: 1812969805\n",
    "<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (b)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bending1\n",
    "raw_bending1_list = []\n",
    "for i in range(1,8):\n",
    "    df = pd.read_csv('../data/AReM/bending1/dataset'+str(i)+'.csv',\n",
    "                    skiprows = [0,1,2,3],\n",
    "                    index_col=None)\n",
    "    df.columns = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23']\n",
    "    raw_bending1_list.append(df)\n",
    "raw_bending1_list[0]\n",
    "\n",
    "\n",
    "# bending2\n",
    "raw_bending2_list = []\n",
    "for i in range(1,7):\n",
    "    df = pd.read_csv('../data/AReM/bending2/dataset'+str(i)+'.csv',\n",
    "                    skiprows = [0,1,2,3],\n",
    "                    index_col=None)\n",
    "    df.columns = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23']\n",
    "    raw_bending2_list.append(df)\n",
    "raw_bending2_list[1]\n",
    "\n",
    "\n",
    "# cycling\n",
    "raw_cycling_list = []\n",
    "for i in range(1,16):\n",
    "    df = pd.read_csv('../data/AReM/cycling/dataset'+str(i)+'.csv',\n",
    "                    skiprows = [0,1,2,3],\n",
    "                    index_col=None)\n",
    "    df.columns = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23']\n",
    "    raw_cycling_list.append(df)\n",
    "raw_cycling_list[0]\n",
    "\n",
    "\n",
    "# lying\n",
    "raw_lying_list = []\n",
    "for i in range(1,16):\n",
    "    df = pd.read_csv('../data/AReM/lying/dataset'+str(i)+'.csv',\n",
    "                    skiprows = [0,1,2,3],\n",
    "                    index_col=None)\n",
    "    df.columns = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23']\n",
    "    raw_lying_list.append(df)\n",
    "raw_lying_list[0]\n",
    "\n",
    "\n",
    "# sitting\n",
    "raw_sitting_list = []\n",
    "for i in range(1,16):\n",
    "    df = pd.read_csv('../data/AReM/sitting/dataset'+str(i)+'.csv',\n",
    "                    skiprows = [0,1,2,3],\n",
    "                    index_col=None)\n",
    "    df.columns = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23']\n",
    "    raw_sitting_list.append(df)\n",
    "raw_sitting_list[1]\n",
    "\n",
    "\n",
    "# standing\n",
    "raw_standing_list = []\n",
    "for i in range(1,16):\n",
    "    df = pd.read_csv('../data/AReM/standing/dataset'+str(i)+'.csv',\n",
    "                    skiprows = [0,1,2,3],\n",
    "                    index_col=None)\n",
    "    df.columns = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23']\n",
    "    raw_standing_list.append(df)\n",
    "raw_standing_list[1]\n",
    "\n",
    "\n",
    "# walking\n",
    "raw_walking_list = []\n",
    "for i in range(1,16):\n",
    "    df = pd.read_csv('../data/AReM/walking/dataset'+str(i)+'.csv',\n",
    "                    skiprows = [0,1,2,3],\n",
    "                    index_col=None)\n",
    "    df.columns = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23']\n",
    "    raw_walking_list.append(df)\n",
    "raw_walking_list[1]\n",
    "\n",
    "\n",
    "#num_of_rows = df.iloc[:,0].size\n",
    "#num_of_cols = df.iloc[0,:].size\n",
    "\n",
    "# -------------------split train and test------------------------\n",
    "raw_bending1_list_test = raw_bending1_list[0:2]\n",
    "raw_bending1_list_train = raw_bending1_list[2:]\n",
    "raw_bending2_list_test = raw_bending2_list[0:2]\n",
    "raw_bending2_list_train = raw_bending2_list[2:]\n",
    "raw_cycling_list_test = raw_cycling_list[0:3]\n",
    "raw_cycling_list_train = raw_cycling_list[3:]\n",
    "raw_lying_list_test = raw_lying_list[0:3]\n",
    "raw_lying_list_train = raw_lying_list[3:]\n",
    "raw_sitting_list_test = raw_sitting_list[0:3]\n",
    "raw_sitting_list_train = raw_sitting_list[3:]\n",
    "raw_standing_list_test = raw_standing_list[0:3]\n",
    "raw_standing_list_train = raw_standing_list[3:]\n",
    "raw_walking_list_test = raw_walking_list[0:3]\n",
    "raw_walking_list_train = raw_walking_list[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (c) i)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (c) ii)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>min1</th>\n",
       "      <th>max1</th>\n",
       "      <th>mean1</th>\n",
       "      <th>median1</th>\n",
       "      <th>std1</th>\n",
       "      <th>1st quart1</th>\n",
       "      <th>3rd quart1</th>\n",
       "      <th>min2</th>\n",
       "      <th>max2</th>\n",
       "      <th>...</th>\n",
       "      <th>std5</th>\n",
       "      <th>1st quart5</th>\n",
       "      <th>3rd quart5</th>\n",
       "      <th>min6</th>\n",
       "      <th>max6</th>\n",
       "      <th>mean6</th>\n",
       "      <th>median6</th>\n",
       "      <th>std6</th>\n",
       "      <th>1st quart6</th>\n",
       "      <th>3rd quart6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>37.25</td>\n",
       "      <td>45.00</td>\n",
       "      <td>40.624792</td>\n",
       "      <td>40.50</td>\n",
       "      <td>1.476967</td>\n",
       "      <td>39.2500</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>...</td>\n",
       "      <td>2.188449</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.570583</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38.00</td>\n",
       "      <td>45.67</td>\n",
       "      <td>42.812812</td>\n",
       "      <td>42.50</td>\n",
       "      <td>1.435550</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>43.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995255</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>34.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.571083</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.601010</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>35.00</td>\n",
       "      <td>47.40</td>\n",
       "      <td>43.954500</td>\n",
       "      <td>44.33</td>\n",
       "      <td>1.558835</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>...</td>\n",
       "      <td>1.999604</td>\n",
       "      <td>35.3625</td>\n",
       "      <td>36.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.493292</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.513506</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33.00</td>\n",
       "      <td>47.75</td>\n",
       "      <td>42.179813</td>\n",
       "      <td>43.50</td>\n",
       "      <td>3.670666</td>\n",
       "      <td>39.1500</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.849448</td>\n",
       "      <td>30.4575</td>\n",
       "      <td>36.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.613521</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.524317</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33.00</td>\n",
       "      <td>45.75</td>\n",
       "      <td>41.678063</td>\n",
       "      <td>41.75</td>\n",
       "      <td>2.243490</td>\n",
       "      <td>41.3300</td>\n",
       "      <td>42.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>...</td>\n",
       "      <td>2.411026</td>\n",
       "      <td>28.4575</td>\n",
       "      <td>31.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.383292</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.389164</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>37.00</td>\n",
       "      <td>48.25</td>\n",
       "      <td>42.516958</td>\n",
       "      <td>42.50</td>\n",
       "      <td>2.195751</td>\n",
       "      <td>41.0000</td>\n",
       "      <td>44.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.12</td>\n",
       "      <td>...</td>\n",
       "      <td>3.623557</td>\n",
       "      <td>12.6275</td>\n",
       "      <td>17.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0.977417</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.853280</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>85</td>\n",
       "      <td>36.25</td>\n",
       "      <td>45.50</td>\n",
       "      <td>42.959354</td>\n",
       "      <td>42.67</td>\n",
       "      <td>1.500878</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>44.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>...</td>\n",
       "      <td>2.702605</td>\n",
       "      <td>14.0000</td>\n",
       "      <td>16.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.748479</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.461152</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>36.00</td>\n",
       "      <td>47.33</td>\n",
       "      <td>42.674583</td>\n",
       "      <td>43.67</td>\n",
       "      <td>2.384170</td>\n",
       "      <td>40.0000</td>\n",
       "      <td>44.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.17</td>\n",
       "      <td>...</td>\n",
       "      <td>3.261617</td>\n",
       "      <td>12.7500</td>\n",
       "      <td>16.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.702042</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.567451</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>36.25</td>\n",
       "      <td>45.75</td>\n",
       "      <td>43.187521</td>\n",
       "      <td>44.75</td>\n",
       "      <td>2.491162</td>\n",
       "      <td>39.7500</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>...</td>\n",
       "      <td>3.566038</td>\n",
       "      <td>16.5000</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.645458</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.567419</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>36.00</td>\n",
       "      <td>47.33</td>\n",
       "      <td>44.441187</td>\n",
       "      <td>45.00</td>\n",
       "      <td>2.417797</td>\n",
       "      <td>44.6275</td>\n",
       "      <td>45.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>...</td>\n",
       "      <td>3.414454</td>\n",
       "      <td>11.0000</td>\n",
       "      <td>14.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.91</td>\n",
       "      <td>1.155083</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.842087</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Instance   min1   max1      mean1  median1      std1  1st quart1  \\\n",
       "0          1  37.25  45.00  40.624792    40.50  1.476967     39.2500   \n",
       "1          2  38.00  45.67  42.812812    42.50  1.435550     42.0000   \n",
       "2          3  35.00  47.40  43.954500    44.33  1.558835     43.0000   \n",
       "3          4  33.00  47.75  42.179813    43.50  3.670666     39.1500   \n",
       "4          5  33.00  45.75  41.678063    41.75  2.243490     41.3300   \n",
       "..       ...    ...    ...        ...      ...       ...         ...   \n",
       "83        84  37.00  48.25  42.516958    42.50  2.195751     41.0000   \n",
       "84        85  36.25  45.50  42.959354    42.67  1.500878     42.0000   \n",
       "85        86  36.00  47.33  42.674583    43.67  2.384170     40.0000   \n",
       "86        87  36.25  45.75  43.187521    44.75  2.491162     39.7500   \n",
       "87        88  36.00  47.33  44.441187    45.00  2.417797     44.6275   \n",
       "\n",
       "    3rd quart1  min2  max2  ...      std5  1st quart5  3rd quart5  min6  max6  \\\n",
       "0        42.00   0.0  1.30  ...  2.188449     33.0000       36.00   0.0  1.92   \n",
       "1        43.67   0.0  1.22  ...  1.995255     32.0000       34.50   0.0  3.11   \n",
       "2        45.00   0.0  1.70  ...  1.999604     35.3625       36.50   0.0  1.79   \n",
       "3        45.00   0.0  3.00  ...  3.849448     30.4575       36.33   0.0  2.18   \n",
       "4        42.75   0.0  2.83  ...  2.411026     28.4575       31.25   0.0  1.79   \n",
       "..         ...   ...   ...  ...       ...         ...         ...   ...   ...   \n",
       "83       44.50   0.0  2.12  ...  3.623557     12.6275       17.50   0.0  6.85   \n",
       "84       44.33   0.0  2.60  ...  2.702605     14.0000       16.69   0.0  4.00   \n",
       "85       44.75   0.0  2.17  ...  3.261617     12.7500       16.50   0.0  3.77   \n",
       "86       45.00   0.0  2.83  ...  3.566038     16.5000       21.00   0.0  3.83   \n",
       "87       45.75   0.0  4.50  ...  3.414454     11.0000       14.67   0.0  5.91   \n",
       "\n",
       "       mean6  median6      std6  1st quart6  3rd quart6  \n",
       "0   0.570583     0.43  0.582915        0.00        1.30  \n",
       "1   0.571083     0.43  0.601010        0.00        1.30  \n",
       "2   0.493292     0.43  0.513506        0.00        0.94  \n",
       "3   0.613521     0.50  0.524317        0.00        1.00  \n",
       "4   0.383292     0.43  0.389164        0.00        0.50  \n",
       "..       ...      ...       ...         ...         ...  \n",
       "83  0.977417     0.83  0.853280        0.47        1.22  \n",
       "84  0.748479     0.82  0.461152        0.43        0.95  \n",
       "85  0.702042     0.50  0.567451        0.43        0.94  \n",
       "86  0.645458     0.50  0.567419        0.43        0.83  \n",
       "87  1.155083     0.94  0.842087        0.50        1.50  \n",
       "\n",
       "[88 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_raw_list = raw_bending1_list+raw_bending2_list+raw_cycling_list+raw_lying_list+raw_sitting_list+raw_standing_list+raw_standing_list\n",
    "# -----------------raw data------------------------\n",
    "#print(len(all_raw_list))\n",
    "result_table_data = []\n",
    "for each_df in all_raw_list:\n",
    "    describe_result = each_df.describe()\n",
    "    describe_result = describe_result.reindex(['count','min','max','mean','50%','std','25%','75%'])\n",
    "    temp_list = []\n",
    "    for col in range(1,7):\n",
    "        for row in range(1,8):\n",
    "            temp1 = describe_result.iloc[row,col]\n",
    "            temp_list.append(temp1)\n",
    "    result_table_data.append(temp_list)        \n",
    "\n",
    "    \n",
    "# create header of result table\n",
    "header = []\n",
    "item_list = ['min','max','mean','median','std','1st quart','3rd quart']\n",
    "for i in range(1,7):\n",
    "    for j in range(0,7):\n",
    "        temp = item_list[j]+str(i)\n",
    "        header.append(temp)\n",
    "    \n",
    "\n",
    "df_result = pd.DataFrame(data=result_table_data, columns=header)\n",
    "df_result.insert(0, \"Instance\", range(1,89))\n",
    "display(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (c) iii)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For feature min:\n",
      "\n",
      "      estimate std                90% Confidence Interval\n",
      "min1      8.397086  [6.696655533796437,9.870799020112534]\n",
      "min2      0.000000                              [0.0,0.0]\n",
      "min3      2.739917  [2.517481808575439,2.967757861619882]\n",
      "min4      0.000000                              [0.0,0.0]\n",
      "min5      6.156647  [4.549094654672446,7.607617002322668]\n",
      "min6      0.000000                              [0.0,0.0]\n",
      "\n",
      "\n",
      "For feature max:\n",
      "\n",
      "      estimate std                  90% Confidence Interval\n",
      "max1      4.328527  [3.0045532251309224,5.1204273379719405]\n",
      "max2      3.662153   [3.1216528189909343,3.993307960960985]\n",
      "max3      4.871872    [4.195738614435027,5.327418434586251]\n",
      "max4      1.955225   [1.7003573396920506,2.129760249184932]\n",
      "max5      5.846554    [4.650901848417756,6.756581141876134]\n",
      "max6      2.231716  [1.9383448004932389,2.5552392473218077]\n",
      "\n",
      "\n",
      "For feature mean:\n",
      "\n",
      "       estimate std                  90% Confidence Interval\n",
      "mean1      5.156060     [4.381632692631764,5.83836971553452]\n",
      "mean2      0.880902  [0.7157469484488292,0.9999195323322624]\n",
      "mean3      4.075190    [3.528853958217516,4.531976025447662]\n",
      "mean4      0.889224  [0.7590144355348284,1.0155738759973036]\n",
      "mean5      5.755561    [4.583845759930662,6.614910756519216]\n",
      "mean6      0.853757  [0.7217442661184055,0.9667010648899211]\n",
      "\n",
      "\n",
      "For feature median:\n",
      "\n",
      "         estimate std                  90% Confidence Interval\n",
      "median1      5.461065     [4.723180023304355,6.10430097043722]\n",
      "median2      0.725521  [0.6011182331000193,0.8007709031028777]\n",
      "median3      4.129917  [3.6062635049233647,4.6629808580943735]\n",
      "median4      0.861501  [0.7190288715194352,0.9989380660551732]\n",
      "median5      5.889924    [4.775343611884934,6.876140615374055]\n",
      "median6      0.778302  [0.6524106996933605,0.8853000247266414]\n",
      "\n",
      "\n",
      "For feature std:\n",
      "\n",
      "      estimate std                   90% Confidence Interval\n",
      "std1      1.581260   [1.3564980862933709,1.8062921555118585]\n",
      "std2      0.593898   [0.4913431666916306,0.6779241043491242]\n",
      "std3      0.963904   [0.7948958719940331,1.1591444320824185]\n",
      "std4      0.391461  [0.3321514646376845,0.43792047423041225]\n",
      "std5      1.038100   [0.8338763027979728,1.1955487005761776]\n",
      "std6      0.431680  [0.3742242608730761,0.46440749754036875]\n",
      "\n",
      "\n",
      "For feature 1st quart:\n",
      "\n",
      "            estimate std                   90% Confidence Interval\n",
      "1st quart1      5.844251     [5.180477243061377,6.444477285791825]\n",
      "1st quart2      0.498512  [0.42834522974711353,0.5665178703424592]\n",
      "1st quart3      4.364730      [3.797274939399643,4.76602516813329]\n",
      "1st quart4      0.611628   [0.5446746288539138,0.6843997504864456]\n",
      "1st quart5      6.153050    [4.7564456619346265,7.415224936957085]\n",
      "1st quart6      0.524715   [0.4477771003292678,0.6047768956065142]\n",
      "\n",
      "\n",
      "For feature 3rd quart:\n",
      "\n",
      "            estimate std                  90% Confidence Interval\n",
      "3rd quart1      5.083943  [3.9596227263719768,6.0046102321250165]\n",
      "3rd quart2      1.234280   [0.9553237082908995,1.439935645144532]\n",
      "3rd quart3      4.219507   [3.6231114701602283,4.675412657669798]\n",
      "3rd quart4      1.201600   [1.005524127339149,1.3481935751336922]\n",
      "3rd quart5      5.642328    [4.457651265015098,6.664155987930953]\n",
      "3rd quart6      1.132139   [0.9388156608415319,1.302366617358993]\n"
     ]
    }
   ],
   "source": [
    "min_df = df_result[['min1','min2','min3','min4','min5','min6']]\n",
    "max_df = df_result[['max1','max2','max3','max4','max5','max6']]\n",
    "mean_df = df_result[['mean1','mean2','mean3','mean4','mean5','mean6']]\n",
    "median_df = df_result[['median1','median2','median3','median4','median5','median6']]\n",
    "std_df = df_result[['std1','std2','std3','std4','std5','std6']]\n",
    "firstq_df = df_result[['1st quart1','1st quart2','1st quart3','1st quart4','1st quart5','1st quart6']]\n",
    "thirdq_df = df_result[['3rd quart1','3rd quart2','3rd quart3','3rd quart4','3rd quart5','3rd quart6']]\n",
    "\n",
    "# calculate bootstrap\n",
    "iterates = 100\n",
    "df_list = [min_df,max_df,mean_df,median_df,std_df,firstq_df,thirdq_df]\n",
    "df_name = ['min','max','mean','median','std','1st quart','3rd quart']\n",
    "\n",
    "for k in range(0,7):\n",
    "    print('\\n\\nFor feature '+df_name[k]+':\\n')\n",
    "    \n",
    "    current_df = df_list[k]\n",
    "    \n",
    "    result_std_df = current_df.std(skipna = True,ddof=0) \n",
    "    result_std_df = result_std_df.to_frame()\n",
    "    result_std_df.columns = [\"estimate std\"]\n",
    "\n",
    "    \n",
    "    result_interval_list = []\n",
    "    for j in range(1,7):\n",
    "        current_col_name = df_name[k]+str(j)\n",
    "        current_col = current_df[current_col_name]\n",
    "        std_interval = []\n",
    "        for i in range(0,iterates):\n",
    "            current_sample = np.random.choice(a=list(current_col), size=len(list(current_col)), replace=True)\n",
    "            current_std = np.std(current_sample)\n",
    "            std_interval.append(current_std)\n",
    "        std_interval.sort()\n",
    "        #result_interval = std_interval[5:95]\n",
    "        result_interval_start = std_interval[5]\n",
    "        result_interval_end = std_interval[94]\n",
    "        result_interval = '['+str(result_interval_start)+','+str(result_interval_end)+']'\n",
    "        result_interval_list.append(result_interval)\n",
    "\n",
    "    result_std_df['90% Confidence Interval'] = result_interval_list\n",
    "    print(result_std_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (c) iv)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (d) i)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                time   avg_rss12  var_rss12   avg_rss13   var_rss13  \\\n",
      "count     480.000000  480.000000  480.00000  480.000000  480.000000   \n",
      "min         0.000000   35.000000    0.00000    6.500000    0.000000   \n",
      "max    119750.000000   47.400000    1.70000   29.750000    4.440000   \n",
      "mean    59875.000000   43.954500    0.42625   22.122354    0.497313   \n",
      "50%     59875.000000   44.330000    0.47000   23.000000    0.430000   \n",
      "std     34677.081769    1.558835    0.33869    3.030943    0.550657   \n",
      "25%     29937.500000   43.000000    0.00000   19.750000    0.000000   \n",
      "75%     89812.500000   45.000000    0.50000   24.000000    0.830000   \n",
      "\n",
      "        avg_rss23   var_rss23  \n",
      "count  480.000000  480.000000  \n",
      "min     29.000000    0.000000  \n",
      "max     38.500000    1.790000  \n",
      "mean    35.588458    0.493292  \n",
      "50%     36.000000    0.430000  \n",
      "std      1.999604    0.513506  \n",
      "25%     35.362500    0.000000  \n",
      "75%     36.500000    0.940000  \n",
      "                time   avg_rss12   var_rss12   avg_rss13   var_rss13  \\\n",
      "count     480.000000  480.000000  480.000000  480.000000  480.000000   \n",
      "min         0.000000   33.000000    0.000000    8.500000    0.000000   \n",
      "max    119750.000000   47.750000    3.000000   30.000000    5.150000   \n",
      "mean    59875.000000   42.179813    0.696042   22.183625    0.989917   \n",
      "50%     59875.000000   43.500000    0.500000   23.000000    0.830000   \n",
      "std     34677.081769    3.670666    0.630860    3.810469    0.953730   \n",
      "25%     29937.500000   39.150000    0.000000   20.500000    0.430000   \n",
      "75%     89812.500000   45.000000    1.120000   24.372500    1.300000   \n",
      "\n",
      "        avg_rss23   var_rss23  \n",
      "count  480.000000  480.000000  \n",
      "min     20.000000    0.000000  \n",
      "max     38.670000    2.180000  \n",
      "mean    33.493917    0.613521  \n",
      "50%     35.000000    0.500000  \n",
      "std      3.849448    0.524317  \n",
      "25%     30.457500    0.000000  \n",
      "75%     36.330000    1.000000  \n",
      "                time   avg_rss12   var_rss12   avg_rss13   var_rss13  \\\n",
      "count     480.000000  480.000000  480.000000  480.000000  480.000000   \n",
      "min         0.000000   33.000000    0.000000    3.000000    0.000000   \n",
      "max    119750.000000   45.750000    2.830000   28.250000    6.420000   \n",
      "mean    59875.000000   41.678063    0.535979   19.006562    0.841875   \n",
      "50%     59875.000000   41.750000    0.500000   19.125000    0.500000   \n",
      "std     34677.081769    2.243490    0.405469    4.087107    0.928801   \n",
      "25%     29937.500000   41.330000    0.430000   16.500000    0.430000   \n",
      "75%     89812.500000   42.750000    0.710000   22.062500    1.120000   \n",
      "\n",
      "        avg_rss23   var_rss23  \n",
      "count  480.000000  480.000000  \n",
      "min     23.670000    0.000000  \n",
      "max     37.500000    1.790000  \n",
      "mean    29.857083    0.383292  \n",
      "50%     30.000000    0.430000  \n",
      "std      2.411026    0.389164  \n",
      "25%     28.457500    0.000000  \n",
      "75%     31.250000    0.500000  \n",
      "                time   avg_rss12   var_rss12   avg_rss13   var_rss13  \\\n",
      "count     480.000000  480.000000  480.000000  480.000000  480.000000   \n",
      "min         0.000000   37.000000    0.000000    5.750000    0.000000   \n",
      "max    119750.000000   48.000000    1.580000   27.000000   10.030000   \n",
      "mean    59875.000000   43.454958    0.378083   15.793333    0.849354   \n",
      "50%     59875.000000   43.250000    0.470000   15.000000    0.500000   \n",
      "std     34677.081769    1.386098    0.315566    3.847638    0.995761   \n",
      "25%     29937.500000   42.500000    0.000000   13.000000    0.430000   \n",
      "75%     89812.500000   45.000000    0.500000   18.270000    1.120000   \n",
      "\n",
      "        avg_rss23   var_rss23  \n",
      "count  480.000000  480.000000  \n",
      "min      8.000000    0.000000  \n",
      "max     33.500000    5.260000  \n",
      "mean    23.034792    0.679646  \n",
      "50%     23.500000    0.500000  \n",
      "std      2.488862    0.622534  \n",
      "25%     22.250000    0.430000  \n",
      "75%     24.000000    0.870000  \n",
      "                time   avg_rss12   var_rss12   avg_rss13   var_rss13  \\\n",
      "count     480.000000  480.000000  480.000000  480.000000  480.000000   \n",
      "min         0.000000   36.250000    0.000000    1.500000    0.000000   \n",
      "max    119750.000000   48.000000    1.500000   26.330000    5.170000   \n",
      "mean    59875.000000   43.969125    0.413125   15.868021    0.666354   \n",
      "50%     59875.000000   44.500000    0.470000   16.250000    0.470000   \n",
      "std     34677.081769    1.618364    0.263111    3.742420    0.788985   \n",
      "25%     29937.500000   43.310000    0.430000   14.250000    0.000000   \n",
      "75%     89812.500000   44.670000    0.500000   18.000000    0.940000   \n",
      "\n",
      "        avg_rss23   var_rss23  \n",
      "count  480.000000  480.000000  \n",
      "min     11.330000    0.000000  \n",
      "max     30.750000    2.960000  \n",
      "mean    22.103750    0.555313  \n",
      "50%     21.670000    0.490000  \n",
      "std      3.318301    0.487826  \n",
      "25%     20.500000    0.000000  \n",
      "75%     23.750000    0.830000  \n"
     ]
    }
   ],
   "source": [
    "train_raw_list = raw_bending1_list_train+raw_bending2_list_train+raw_cycling_list_train+raw_lying_list_train+raw_sitting_list_train+raw_standing_list_train+raw_walking_list_train\n",
    "\n",
    "\n",
    "# -----------------raw data------------------------\n",
    "#print(len(train_raw_list))\n",
    "result_table_data = []\n",
    "for each_df in raw_bending1_list_train:\n",
    "    describe_result = each_df.describe()\n",
    "    describe_result = describe_result.reindex(['count','min','max','mean','50%','std','25%','75%'])\n",
    "    print(describe_result)\n",
    "    temp_list = []\n",
    "    for col in range(1,7):\n",
    "        for row in range(1,8):\n",
    "            temp1 = describe_result.iloc[row,col]\n",
    "            temp_list.append(temp1)\n",
    "    #print(temp_list)\n",
    "    result_table_data.append(temp_list)        \n",
    "\n",
    "#print(result_table_data)\n",
    "    \n",
    "# create header of result table\n",
    "header = []\n",
    "item_list = ['min','max','mean','median','std','1st quart','3rd quart']\n",
    "for i in range(1,7):\n",
    "    for j in range(0,7):\n",
    "        temp = item_list[j]+str(i)\n",
    "        header.append(temp)\n",
    "    \n",
    "\n",
    "#df_result = pd.DataFrame(data=result_table_data, columns=header)\n",
    "#df_result.insert(0, \"Instance\", range(1,70))\n",
    "#display(df_result)\n",
    "\n",
    "#df_result_show = df_result[['max1','std1','mean1','max2','std2','mean2','max6','std6','mean6']] # missing min\n",
    "#df_result_show = df_result[['min1','mean1','max1','min2','mean2','max2','min6','mean6','max6']] # missing std\n",
    "#df_result_show = df_result[['min1','mean1','std1','min2','mean2','std2','min6','mean6','std6']] # missing max\n",
    "#df_result_show = df_result[['min1','std1','max1','min2','std2','max2','min6','std6','max6']] # missing mean\n",
    "\n",
    "#temp = df_result_show.copy()\n",
    "#df_result_show = temp\n",
    "#df_result_show['class']=1\n",
    "#for i in range(0,10):\n",
    "#    df_result_show.iloc[i, 9] = 0\n",
    "#display(df_result_show)\n",
    "#seaborn.pairplot(df_result_show,hue='class') \n",
    "#plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (d) ii)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the nonlinear model for each predictor:\n",
      "PE = -0.61034570831265*AT  +(-0.1251381773994436)*AT^2 +(0.0026748519468189876)*AT^3 +(492.7281433373427)\n",
      "PE = -2.1443773224271254*V  +(-0.0027122848954050927)*V^2 +(0.0001343571097207584)*V^3 +(554.1468490636844)\n",
      "PE = 25.255593121426305*AP  +(-0.0499520697383803)*AP^2 +(2.5135259019581055e-05)*AP^3 +(0.07469356904837113)\n",
      "PE = -1.729211307727816*RH  +(0.03214517214046108)*RH^2 +(-0.00015218796881679876)*RH^3 +(468.41353597116665)\n",
      "\n",
      "Here is P value list for each predictor model:\n",
      "AT: [0.0, 7.898147240186973e-07, 8.833045326883238e-73, 3.652184575690421e-110]\n",
      "V: [0.0, 2.5265890334068745e-05, 0.7684969443856083, 0.013734885927079613]\n",
      "AP: [4.502733627631705e-17, 4.502735428575159e-17, 3.66670487133258e-17, 8.264145644758208e-18]\n",
      "RH: [0.0, 0.0003772509907903172, 9.39542958956797e-06, 1.4402785080640726e-05]\n",
      "\n",
      "For predictor AT,AP and RH, the p value is vary small (much smaller than alpha value).\n",
      "Therefore, there is some nonlinear association between those three predictors and the response.\n",
      "For the predictor V, the p value of term V**2 is greater than 0.05, but the p value of term V**3 is less than 0.05,\n",
      "Therefore, there is some nonlinear association between predictor V and the response.\n"
     ]
    }
   ],
   "source": [
    "#fg = smf.ols(formula='X ~ Y + Y**2', data=data).fit()\n",
    "result_AT = sm.ols(formula=\"PE ~ AT + I(AT**2) + I(AT**3)\", data=df).fit()\n",
    "result_V = sm.ols(formula=\"PE ~ V + I(V**2) + I(V**3)\", data=df).fit()\n",
    "result_AP = sm.ols(formula=\"PE ~ AP + I(AP**2) + I(AP**3)\", data=df).fit()\n",
    "result_RH = sm.ols(formula=\"PE ~ RH + I(RH**2) + I(RH**3)\", data=df).fit()\n",
    "#print(result_AT.summary())\n",
    "#print(result_AT.pvalues)\n",
    "\n",
    "print('Here are the nonlinear model for each predictor:')\n",
    "temp = list(result_AT.params)\n",
    "print('PE = '+str(temp[1])+'*AT  +('+str(temp[2])+')*AT^2 +(' +str(temp[3])+')*AT^3 +(' +(str(temp[0]))+')')\n",
    "temp = list(result_V.params)\n",
    "print('PE = '+str(temp[1])+'*V  +('+str(temp[2])+')*V^2 +(' +str(temp[3])+')*V^3 +(' +(str(temp[0]))+')')\n",
    "temp = list(result_AP.params)\n",
    "print('PE = '+str(temp[1])+'*AP  +('+str(temp[2])+')*AP^2 +(' +str(temp[3])+')*AP^3 +(' +(str(temp[0]))+')')\n",
    "temp = list(result_RH.params)\n",
    "print('PE = '+str(temp[1])+'*RH  +('+str(temp[2])+')*RH^2 +(' +str(temp[3])+')*RH^3 +(' +(str(temp[0]))+')')\n",
    "\n",
    "print('\\nHere is P value list for each predictor model:')\n",
    "print('AT: '+str(list(result_AT.pvalues)))\n",
    "print('V: '+str(list(result_V.pvalues)))\n",
    "print('AP: '+str(list(result_AP.pvalues)))\n",
    "print('RH: '+str(list(result_RH.pvalues)))\n",
    "#print(result_V.summary())\n",
    "\n",
    "print('\\nFor predictor AT,AP and RH, the p value is vary small (much smaller than alpha value).')\n",
    "print('Therefore, there is some nonlinear association between those three predictors and the response.')\n",
    "print('For the predictor V, the p value of term V**2 is greater than 0.05, but the p value of term V**3 is less than 0.05,')\n",
    "print('Therefore, there is some nonlinear association between predictor V and the response.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (g)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the summary table of the model:\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     PE   R-squared:                       0.936\n",
      "Model:                            OLS   Adj. R-squared:                  0.936\n",
      "Method:                 Least Squares   F-statistic:                 1.405e+04\n",
      "Date:                Fri, 25 Sep 2020   Prob (F-statistic):               0.00\n",
      "Time:                        23:46:34   Log-Likelihood:                -27548.\n",
      "No. Observations:                9568   AIC:                         5.512e+04\n",
      "Df Residuals:                    9557   BIC:                         5.520e+04\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    685.7825     78.640      8.721      0.000     531.631     839.934\n",
      "AT            -4.3470      2.373     -1.832      0.067      -8.999       0.305\n",
      "V             -7.6749      1.351     -5.682      0.000     -10.323      -5.027\n",
      "AP            -0.1524      0.077     -1.983      0.047      -0.303      -0.002\n",
      "RH             1.5709      0.773      2.031      0.042       0.055       3.087\n",
      "AT:V           0.0210      0.001     23.338      0.000       0.019       0.023\n",
      "AT:AP          0.0018      0.002      0.752      0.452      -0.003       0.006\n",
      "AT:RH         -0.0052      0.001     -6.444      0.000      -0.007      -0.004\n",
      "V:AP           0.0068      0.001      5.135      0.000       0.004       0.009\n",
      "V:RH           0.0008      0.000      1.716      0.086      -0.000       0.002\n",
      "AP:RH         -0.0016      0.001     -2.125      0.034      -0.003      -0.000\n",
      "==============================================================================\n",
      "Omnibus:                     1454.609   Durbin-Watson:                   2.030\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             9170.848\n",
      "Skew:                          -0.574   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.657   Cond. No.                     1.70e+08\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.7e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "From the table we see that the p value of AP, AT*AP and V*RH are greater than 0.05.\n",
      "So we accept null hypothesis for AP, AT*AP and V*RH and reject null hypothesis for others.\n",
      "Therefore, AT*V, AT*RH, V*AP, AP*RH have association with the response.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "predictor_df = df.iloc[:, :-1].values\n",
    "poly = PolynomialFeatures(degree = 2,interaction_only=True)\n",
    "temp = poly.fit_transform(predictor_df)\n",
    "df_interaction = pd.DataFrame(temp, columns=['const','AT', 'V', 'AP', 'RH', 'AT*V', 'AT*AP', 'AT*RH', 'V*AP', 'V*RH', 'AP*RH'])\n",
    "#df_interaction = pd.DataFrame(temp, columns=['const','AT', 'V', 'AP', 'RH', 'AT_V', 'AT_AP', 'AT_RH', 'V_AP', 'V_RH', 'AP_RH'])\n",
    "temp = df[\"PE\"]\n",
    "df_interaction['PE'] = temp\n",
    "#df_interaction\n",
    "\n",
    "result_interaction = sm.ols(formula=\"PE ~ AT + V + AP + RH + AT*V + AT*AP + AT*RH + V*AP + V*RH + AP*RH\", data=df_interaction).fit()\n",
    "#result_interaction = sm.ols(formula=\"PE ~ AT + V + AP + RH + AT_V + AT_AP + AT_RH + V_AP + V_RH + AP_RH\", data=df_interaction).fit()\n",
    "print('Here is the summary table of the model:\\n')\n",
    "print(result_interaction.summary())\n",
    "\n",
    "print('From the table we see that the p value of AP, AT*AP and V*RH are greater than 0.05.')\n",
    "print('So we accept null hypothesis for AP, AT*AP and V*RH and reject null hypothesis for others.')\n",
    "print('Therefore, AT*V, AT*RH, V*AP, AP*RH have association with the response.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (h)<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>   My improved model part<h7>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     PE   R-squared:                       0.938\n",
      "Model:                            OLS   Adj. R-squared:                  0.938\n",
      "Method:                 Least Squares   F-statistic:                     5940.\n",
      "Date:                Fri, 25 Sep 2020   Prob (F-statistic):               0.00\n",
      "Time:                        23:46:35   Log-Likelihood:                -19202.\n",
      "No. Observations:                6697   AIC:                         3.844e+04\n",
      "Df Residuals:                    6679   BIC:                         3.856e+04\n",
      "Df Model:                          17                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.0227      0.004     -5.127      0.000      -0.031      -0.014\n",
      "AT            -0.7473      3.711     -0.201      0.840      -8.021       6.527\n",
      "V             -2.9353      1.810     -1.621      0.105      -6.484       0.614\n",
      "AP            -7.6630      1.494     -5.130      0.000     -10.591      -4.735\n",
      "RH             5.7242      1.067      5.363      0.000       3.632       7.817\n",
      "AT:V           0.0126      0.003      3.816      0.000       0.006       0.019\n",
      "AT:AP         -0.0009      0.004     -0.248      0.804      -0.008       0.006\n",
      "AT:RH         -0.0068      0.002     -3.803      0.000      -0.010      -0.003\n",
      "V:AP           0.0051      0.002      2.879      0.004       0.002       0.009\n",
      "V:RH           0.0011      0.001      1.421      0.155      -0.000       0.003\n",
      "AP:RH         -0.0052      0.001     -5.108      0.000      -0.007      -0.003\n",
      "I(AT ** 2)    -0.0423      0.008     -5.278      0.000      -0.058      -0.027\n",
      "I(V ** 2)     -0.0485      0.006     -7.774      0.000      -0.061      -0.036\n",
      "I(AP ** 2)     0.0162      0.003      5.608      0.000       0.011       0.022\n",
      "I(RH ** 2)    -0.0040      0.002     -1.636      0.102      -0.009       0.001\n",
      "I(AT ** 3)     0.0010      0.000      7.249      0.000       0.001       0.001\n",
      "I(V ** 3)      0.0003   3.62e-05      7.508      0.000       0.000       0.000\n",
      "I(AP ** 3) -8.092e-06    1.4e-06     -5.783      0.000   -1.08e-05   -5.35e-06\n",
      "I(RH ** 3)  6.758e-06   1.17e-05      0.580      0.562   -1.61e-05    2.96e-05\n",
      "==============================================================================\n",
      "Omnibus:                     1472.686   Durbin-Watson:                   1.986\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            12613.221\n",
      "Skew:                          -0.811   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.525   Cond. No.                     2.19e+15\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.19e+15. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "\n",
      "From the table above we see that the p value of AT, V, AT*AP, V*RH, RH**2, RH**3 are greater than 0.05 (alpha value).\n",
      "Therefore, we accepy null hypothesis and remove those terms. Then we get the following result summary for my model:\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     PE   R-squared:                       0.937\n",
      "Model:                            OLS   Adj. R-squared:                  0.937\n",
      "Method:                 Least Squares   F-statistic:                     7686.\n",
      "Date:                Fri, 25 Sep 2020   Prob (F-statistic):               0.00\n",
      "Time:                        23:46:35   Log-Likelihood:                -19237.\n",
      "No. Observations:                6697   AIC:                         3.850e+04\n",
      "Df Residuals:                    6683   BIC:                         3.860e+04\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.0195      0.004     -5.085      0.000      -0.027      -0.012\n",
      "AP            -6.5995      1.297     -5.087      0.000      -9.143      -4.056\n",
      "RH             3.0043      0.804      3.739      0.000       1.429       4.579\n",
      "AT            -2.3529      0.166    -14.137      0.000      -2.679      -2.027\n",
      "V             -3.6765      1.132     -3.247      0.001      -5.896      -1.457\n",
      "AT:V           0.0115      0.002      4.616      0.000       0.007       0.016\n",
      "AT:RH          0.0003      0.001      0.373      0.709      -0.001       0.002\n",
      "V:AP           0.0058      0.001      5.402      0.000       0.004       0.008\n",
      "AP:RH         -0.0031      0.001     -3.907      0.000      -0.005      -0.002\n",
      "I(AT ** 2)    -0.0344      0.008     -4.471      0.000      -0.049      -0.019\n",
      "I(V ** 2)     -0.0467      0.006     -7.498      0.000      -0.059      -0.035\n",
      "I(AP ** 2)     0.0144      0.003      5.705      0.000       0.009       0.019\n",
      "I(AT ** 3)     0.0010      0.000      6.940      0.000       0.001       0.001\n",
      "I(V ** 3)      0.0003   3.62e-05      7.229      0.000       0.000       0.000\n",
      "I(AP ** 3) -7.302e-06   1.23e-06     -5.960      0.000    -9.7e-06    -4.9e-06\n",
      "==============================================================================\n",
      "Omnibus:                     1445.661   Durbin-Watson:                   1.984\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            12217.868\n",
      "Skew:                          -0.796   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.423   Cond. No.                     2.19e+15\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.19e+15. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "After improve my model, I get following error data:\n",
      "Test error: 16.981716087934156\n",
      "Train error: 18.301473903436047\n"
     ]
    }
   ],
   "source": [
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sma\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_df, test_df = train_test_split(df, test_size=0.3,random_state=32)\n",
    "test_df_data = test_df[['AT','V','AP','RH']]\n",
    "test_df_data = sma.add_constant(test_df_data)\n",
    "test_df_label = test_df[['PE']]\n",
    "train_df_data = train_df[['AT','V','AP','RH']]\n",
    "train_df_data = sma.add_constant(train_df_data)\n",
    "train_df_label = train_df[['PE']]\n",
    "# regression on all possible interaction term and quadratic \n",
    "\n",
    "result_all_possible = sm.ols(formula=\"PE ~ AT + V + AP + RH + AT*V + AT*AP + AT*RH + V*AP + V*RH + AP*RH + I(AT**2) + I(V**2) + I(AP**2) + I(RH**2)+ I(AT**3) + I(V**3) + I(AP**3) + I(RH**3)\", data = train_df).fit()\n",
    "print(result_all_possible.summary())\n",
    "print('\\n\\nFrom the table above we see that the p value of AT, V, AT*AP, V*RH, RH**2, RH**3 are greater than 0.05 (alpha value).')\n",
    "print('Therefore, we accepy null hypothesis and remove those terms. Then we get the following result summary for my model:\\n')\n",
    "result_all_possible = sm.ols(formula=\"PE ~ AP + RH + AT*V + AT*RH + V*AP + AP*RH + I(AT**2) + I(V**2) + I(AP**2) + I(AT**3) + I(V**3) + I(AP**3)\", data = train_df).fit()\n",
    "print(result_all_possible.summary())\n",
    "\n",
    "predictions = result_all_possible.predict(test_df_data)\n",
    "test_error = mean_squared_error(test_df_label, predictions)\n",
    "predictions = result_all_possible.predict(train_df_data)\n",
    "train_error = mean_squared_error(train_df_label, predictions)\n",
    "print('After improve my model, I get following error data:')\n",
    "print('Test error: '+str(test_error))\n",
    "print('Train error: '+str(train_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>   All possible interaction terms and quadratic nonlinearities model part<h7>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     PE   R-squared:                       0.937\n",
      "Model:                            OLS   Adj. R-squared:                  0.937\n",
      "Method:                 Least Squares   F-statistic:                     7059.\n",
      "Date:                Fri, 25 Sep 2020   Prob (F-statistic):               0.00\n",
      "Time:                        23:46:35   Log-Likelihood:                -19271.\n",
      "No. Observations:                6697   AIC:                         3.857e+04\n",
      "Df Residuals:                    6682   BIC:                         3.867e+04\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept  -7722.4222   1470.165     -5.253      0.000   -1.06e+04   -4840.430\n",
      "AT            -0.7164      3.731     -0.192      0.848      -8.030       6.597\n",
      "V             -5.3659      1.815     -2.956      0.003      -8.924      -1.808\n",
      "AP            15.9897      2.850      5.610      0.000      10.403      21.577\n",
      "RH             5.2593      1.071      4.912      0.000       3.161       7.358\n",
      "AT:V           0.0164      0.003      5.103      0.000       0.010       0.023\n",
      "AT:AP         -0.0018      0.004     -0.489      0.625      -0.009       0.005\n",
      "AT:RH         -0.0082      0.002     -4.541      0.000      -0.012      -0.005\n",
      "V:AP           0.0048      0.002      2.743      0.006       0.001       0.008\n",
      "V:RH           0.0013      0.001      1.587      0.113      -0.000       0.003\n",
      "AP:RH         -0.0049      0.001     -4.714      0.000      -0.007      -0.003\n",
      "I(AT ** 2)     0.0110      0.004      3.006      0.003       0.004       0.018\n",
      "I(V ** 2)     -0.0024      0.001     -2.577      0.010      -0.004      -0.001\n",
      "I(AP ** 2)    -0.0078      0.001     -5.615      0.000      -0.010      -0.005\n",
      "I(RH ** 2)    -0.0024      0.000     -7.418      0.000      -0.003      -0.002\n",
      "==============================================================================\n",
      "Omnibus:                     1461.974   Durbin-Watson:                   1.985\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            12378.910\n",
      "Skew:                          -0.807   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.462   Cond. No.                     2.88e+10\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.88e+10. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "\n",
      "From the table above we see that the p value of AT, AT*AP, V*RH are greater than 0.05 (alpha value).\n",
      "Therefore, we accepy null hypothesis and remove those terms.\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     PE   R-squared:                       0.937\n",
      "Model:                            OLS   Adj. R-squared:                  0.937\n",
      "Method:                 Least Squares   F-statistic:                     8234.\n",
      "Date:                Fri, 25 Sep 2020   Prob (F-statistic):               0.00\n",
      "Time:                        23:46:35   Log-Likelihood:                -19272.\n",
      "No. Observations:                6697   AIC:                         3.857e+04\n",
      "Df Residuals:                    6684   BIC:                         3.866e+04\n",
      "Df Model:                          12                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept  -7502.8482   1286.232     -5.833      0.000      -1e+04   -4981.424\n",
      "V             -4.2976      1.111     -3.868      0.000      -6.476      -2.120\n",
      "AP            15.5646      2.502      6.220      0.000      10.659      20.470\n",
      "RH             4.8680      0.859      5.670      0.000       3.185       6.551\n",
      "AT            -2.6549      0.130    -20.393      0.000      -2.910      -2.400\n",
      "AT:V           0.0131      0.002      5.432      0.000       0.008       0.018\n",
      "AT:RH         -0.0058      0.001     -6.012      0.000      -0.008      -0.004\n",
      "V:AP           0.0039      0.001      3.569      0.000       0.002       0.006\n",
      "AP:RH         -0.0045      0.001     -5.445      0.000      -0.006      -0.003\n",
      "I(AT ** 2)     0.0148      0.003      5.683      0.000       0.010       0.020\n",
      "I(V ** 2)     -0.0018      0.001     -2.099      0.036      -0.003      -0.000\n",
      "I(AP ** 2)    -0.0076      0.001     -6.203      0.000      -0.010      -0.005\n",
      "I(RH ** 2)    -0.0022      0.000     -7.476      0.000      -0.003      -0.002\n",
      "==============================================================================\n",
      "Omnibus:                     1462.043   Durbin-Watson:                   1.985\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            12325.907\n",
      "Skew:                          -0.808   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.447   Cond. No.                     2.52e+10\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.52e+10. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "For all possible interaction terms and quadratic nonlinearities model, I get following error data:\n",
      "Test error: 17.241086329644332\n",
      "Train error: 18.4948177618184\n"
     ]
    }
   ],
   "source": [
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sma\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_df, test_df = train_test_split(df, test_size=0.3,random_state=32)\n",
    "test_df_data = test_df[['AT','V','AP','RH']]\n",
    "test_df_data = sma.add_constant(test_df_data)\n",
    "test_df_label = test_df[['PE']]\n",
    "train_df_data = train_df[['AT','V','AP','RH']]\n",
    "train_df_data = sma.add_constant(train_df_data)\n",
    "train_df_label = train_df[['PE']]\n",
    "# regression on all possible interaction term and quadratic \n",
    "\n",
    "result_all_possible = sm.ols(formula=\"PE ~ AT + V + AP + RH + AT*V + AT*AP + AT*RH + V*AP + V*RH + AP*RH + I(AT**2) + I(V**2) + I(AP**2) + I(RH**2)\", data = train_df).fit()\n",
    "print(result_all_possible.summary())\n",
    "print('\\n\\nFrom the table above we see that the p value of AT, AT*AP, V*RH are greater than 0.05 (alpha value).')\n",
    "print('Therefore, we accepy null hypothesis and remove those terms.\\n')\n",
    "result_all_possible = sm.ols(formula=\"PE ~ V + AP + RH + AT*V + AT*RH + V*AP + AP*RH + I(AT**2) + I(V**2) + I(AP**2) + I(RH**2)\", data = train_df).fit()\n",
    "print(result_all_possible.summary())\n",
    "\n",
    "predictions = result_all_possible.predict(test_df_data)\n",
    "test_error = mean_squared_error(test_df_label, predictions)\n",
    "predictions = result_all_possible.predict(train_df_data)\n",
    "train_error = mean_squared_error(train_df_label, predictions)\n",
    "print('For all possible interaction terms and quadratic nonlinearities model, I get following error data:')\n",
    "print('Test error: '+str(test_error))\n",
    "print('Train error: '+str(train_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (i)<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>   Raw data<h7>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For raw data, the optimal k is 5 with MSE = 15.482806048066879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# raw data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_raw_df, test_raw_df = train_test_split(df, test_size=0.3,random_state=32)\n",
    "\n",
    "\n",
    "train_df_data = train_raw_df.iloc[:, :-1].values\n",
    "train_df_label = train_raw_df.iloc[:, 4].values\n",
    "test_df_data = test_raw_df.iloc[:, :-1].values\n",
    "test_df_label = test_raw_df.iloc[:, 4].values\n",
    "#print(test_df.iloc[:,0].size)\n",
    "\n",
    "list_of_k = np.linspace(1, 100, 100)\n",
    "\n",
    "test_error_list = []\n",
    "train_error_list = []\n",
    "optimal_k = list_of_k[0]\n",
    "min_error = math.inf\n",
    "\n",
    "for i in list_of_k:\n",
    "    k = int(i)\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(train_df_data,train_df_label)\n",
    "\n",
    "    # Test error calculation\n",
    "    label_predict = knn.predict(test_df_data)\n",
    "    test_error = mean_squared_error(test_df_label, label_predict)\n",
    "    test_error_list.append(test_error)\n",
    "    if test_error<min_error:\n",
    "        optimal_k = k\n",
    "        min_error = test_error\n",
    "\n",
    "    # train error calculation\n",
    "    label_predict = knn.predict(train_df_data)\n",
    "    train_error = mean_squared_error(train_df_label, label_predict)\n",
    "    train_error_list.append(train_error)\n",
    "\n",
    "    \n",
    "#print(test_error_list)\n",
    "#print(train_error_list)\n",
    "k_inverse_list = []\n",
    "for x in list_of_k:\n",
    "    k_inverse_list.append(1/x)\n",
    "    \n",
    "fig = plt.plot(k_inverse_list, test_error_list, label = \"test error\")\n",
    "fig = plt.plot(k_inverse_list, train_error_list, label = \"train error\")\n",
    "plt.xlabel('1/k')\n",
    "plt.ylabel('error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"For raw data, the optimal k is \"+str(optimal_k)+' with MSE = '+str(min_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For normalized data, the optimal k is 4 with MSE = 13.326641388018109\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# normalized data\n",
    "temp = df[['AT', 'V', 'AP', 'RH']]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(temp)\n",
    "df_normalized = pd.DataFrame(x_scaled,columns=['AT', 'V', 'AP', 'RH'])\n",
    "df_normalized['PE'] = df['PE']\n",
    "\n",
    "\n",
    "\n",
    "train_normalized_df, test_normalized_df = train_test_split(df_normalized, test_size=0.3,random_state=32)\n",
    "\n",
    "\n",
    "train_df_data = train_normalized_df.iloc[:, :-1].values\n",
    "train_df_label = train_normalized_df.iloc[:, 4].values\n",
    "test_df_data = test_normalized_df.iloc[:, :-1].values\n",
    "test_df_label = test_normalized_df.iloc[:, 4].values\n",
    "\n",
    "#print(test_df.iloc[:,0].size)\n",
    "\n",
    "list_of_k = np.linspace(1, 100, 100)\n",
    "\n",
    "test_error_list = []\n",
    "train_error_list = []\n",
    "optimal_k = list_of_k[0]\n",
    "min_error = math.inf\n",
    "\n",
    "for i in list_of_k:\n",
    "    k = int(i)\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(train_df_data,train_df_label)\n",
    "\n",
    "    # Test error calculation\n",
    "    label_predict = knn.predict(test_df_data)\n",
    "    test_error = mean_squared_error(test_df_label, label_predict)\n",
    "    test_error_list.append(test_error)\n",
    "    if test_error<min_error:\n",
    "        optimal_k = k\n",
    "        min_error = test_error\n",
    "\n",
    "    # train error calculation\n",
    "    label_predict = knn.predict(train_df_data)\n",
    "    train_error = mean_squared_error(train_df_label, label_predict)\n",
    "    train_error_list.append(train_error)\n",
    "\n",
    "    \n",
    "    \n",
    "k_inverse_list = []\n",
    "for x in list_of_k:\n",
    "    k_inverse_list.append(1/x)\n",
    "    \n",
    "fig = plt.plot(k_inverse_list, test_error_list, label = \"test error\")\n",
    "fig = plt.plot(k_inverse_list, train_error_list, label = \"train error\")\n",
    "plt.xlabel('1/k')\n",
    "plt.ylabel('error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"For normalized data, the optimal k is \"+str(optimal_k)+' with MSE = '+str(min_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>part (j)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the above calculation, the KNN regression has min error = 13.326641388018109\n",
      "linear regression for each predictor has min error = 20.778255734737538\n",
      "multiple linear regression has min error:\n",
      "AT: 29.437855408963276\n",
      "V: 70.9294774259713\n",
      "AP: 213.01702680361203\n",
      "RH: 247.0508691609319)\n",
      "For this dataset, KNN regression perform best, since there are obvious clusters for predictors AP and RH.\n"
     ]
    }
   ],
   "source": [
    "print('According to the above calculation, the KNN regression has min error = 13.326641388018109')\n",
    "print('linear regression for each predictor has min error = 20.778255734737538')\n",
    "print('multiple linear regression has min error:')\n",
    "print('AT: 29.437855408963276')\n",
    "print('V: 70.9294774259713')\n",
    "print('AP: 213.01702680361203')\n",
    "print('RH: 247.0508691609319)')\n",
    "print('For this dataset, KNN regression perform best, since there are obvious clusters for predictors AP and RH.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>ISLR 2.4.1<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h7>\n",
    "a) A flexible method will perform better than an inflexible method. Since the sample size is very large, the overfitting risk will be very low. Because the number of predictors is small, which means the factors that affect the prediction will be small, a flexible method will produce a more accurate result.<br>\n",
    "b) A flexible method will perform worse than an inflexible method. Since the number of observations is small and the number of predictors is extremely large, the probability of overfitting is relatively high if a flexible method is applied. <br>\n",
    "c) A flexible method will perform better than an inflexible method. A flexible method will produce a more accurate prediction and fit data better because the predictors and response is highly non-linear. <br>\n",
    "d) A flexible method will perform worse than an inflexible method. Since the variance is extremely high, the noise in the data is high. The flexible method will be affected more by noise than inflexible method. <br>\n",
    "<h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>ISLR 2.4.7<h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>a)<h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between observation 1 and the test point is 3.0\n",
      "The Euclidean distance between observation 2 and the test point is 2.0\n",
      "The Euclidean distance between observation 3 and the test point is 3.1622776601683795\n",
      "The Euclidean distance between observation 4 and the test point is 2.23606797749979\n",
      "The Euclidean distance between observation 5 and the test point is 1.4142135623730951\n",
      "The Euclidean distance between observation 6 and the test point is 1.7320508075688772\n"
     ]
    }
   ],
   "source": [
    "distance1 = math.sqrt((0-0)**2+(0-3)**2+(0-0)**2)\n",
    "distance_list = []\n",
    "obs = [[0,3,0],[2,0,0],[0,1,3],[0,1,2],[-1,0,1],[1,1,1]]\n",
    "for i in range(0,6):\n",
    "    distance = math.sqrt((0-obs[i][0])**2+(0-obs[i][1])**2+(0-obs[i][2])**2)\n",
    "    print('The Euclidean distance between observation '+ str(i+1) +' and the test point is '+str(distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>b)<h5>\n",
    "<h7>When k=1, the prediction will be Green. The observation 5 is the closest obervation to the test point. Because k=1, the prediction of test point will be same as the label of its closest point.<h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>c)<h5>\n",
    "<h7>\n",
    "When k=3, the prediction will be Red. The 3 closest observation of test point is observation 2,5,and 6. The majority polling of those observation labels is Red. \n",
    "<h7>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>d)<h5>\n",
    "<h7>\n",
    "The best value of k will be small. As k become larger, the decision boundary will become less flexible. Since the decision boundary is highly non-linear, a more flexible boundary is needed, so we choose small k.\n",
    "<h7>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
